{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bbb4369",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark import SparkConf,sql\n",
    "from pyspark.sql.session import SparkSession\n",
    "import findspark\n",
    "import pymysql\n",
    "import pymysql.cursors\n",
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import datediff, current_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c80c2f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_path='D:/fp5/fp-new/Code/Database/product.csv'\n",
    "cosin_result_path1='D:/fp5/fp-new/Code/cosin_similarity_registry/2024_02_20/cosin_similarity_2024-01-24.csv'\n",
    "cosin_result_path2='D:/fp5/fp-new/Code/cosin_similarity_registry/2024_02_21/cosin_similarity_2024-02-21.csv'\n",
    "cosin_result_path3='D:/fp5/fp-new/Code/cosin_similarity_registry/2024_02_27/cosin_similarity_2024-02-27.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d856acc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark MySQL Example\") \\\n",
    "    .config(\"spark.driver.extraClassPath\", \"C:/spark/spark-3.5.0-bin-hadoop3/jars/mysql-connector-java-5.1.46.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f45dc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Product_df=spark.read.format('csv').option('header',True) \\\n",
    ".load(product_path)\n",
    "\n",
    "cosin_result_path_df1=spark.read.format('csv').option('header',True) \\\n",
    ".load(cosin_result_path1)\n",
    "\n",
    "cosin_result_path_df3=spark.read.format('csv').option('header',True) \\\n",
    ".load(cosin_result_path3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87db2e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-------------------+----------------+---------------+-----------+\n",
      "|product_id|       product_title|       published_at|original_product|matched_product|cosin_score|\n",
      "+----------+--------------------+-------------------+----------------+---------------+-----------+\n",
      "|      3414|RAY SMOCKED CAMI TOP|2020-02-03 00:00:00|            3414|           1436| 0.80962425|\n",
      "|      3414|RAY SMOCKED CAMI TOP|2020-02-03 00:00:00|            3414|           5325| 0.75986445|\n",
      "|      3414|RAY SMOCKED CAMI TOP|2020-02-03 00:00:00|            3414|           7762| 0.87109107|\n",
      "|      3414|RAY SMOCKED CAMI TOP|2020-02-03 00:00:00|            3414|           7650|  0.7890322|\n",
      "|      3414|RAY SMOCKED CAMI TOP|2020-02-03 00:00:00|            3414|            944| 0.79799986|\n",
      "|      3414|RAY SMOCKED CAMI TOP|2020-02-03 00:00:00|            3414|           1361| 0.83114636|\n",
      "|      3414|RAY SMOCKED CAMI TOP|2020-02-03 00:00:00|            3414|           1870| 0.79573506|\n",
      "|      3414|RAY SMOCKED CAMI TOP|2020-02-03 00:00:00|            3414|           7378| 0.79942334|\n",
      "|      3414|RAY SMOCKED CAMI TOP|2020-02-03 00:00:00|            3414|           5316| 0.76143074|\n",
      "|      3414|RAY SMOCKED CAMI TOP|2020-02-03 00:00:00|            3414|           2930|  0.8344438|\n",
      "|      3414|RAY SMOCKED CAMI TOP|2020-02-03 00:00:00|            3414|           5320|  0.7631105|\n",
      "|      3414|RAY SMOCKED CAMI TOP|2020-02-03 00:00:00|            3414|           6380|  0.6719129|\n",
      "|      3414|RAY SMOCKED CAMI TOP|2020-02-03 00:00:00|            3414|           1953|  0.8034643|\n",
      "|      3414|RAY SMOCKED CAMI TOP|2020-02-03 00:00:00|            3414|             51|  0.7756039|\n",
      "|      3414|RAY SMOCKED CAMI TOP|2020-02-03 00:00:00|            3414|            124| 0.76121664|\n",
      "|      3414|RAY SMOCKED CAMI TOP|2020-02-03 00:00:00|            3414|           6820| 0.76091594|\n",
      "|      3414|RAY SMOCKED CAMI TOP|2020-02-03 00:00:00|            3414|           1773|  0.6056395|\n",
      "|      3414|RAY SMOCKED CAMI TOP|2020-02-03 00:00:00|            3414|           1445|  0.5720467|\n",
      "|      3414|RAY SMOCKED CAMI TOP|2020-02-03 00:00:00|            3414|           4417|  0.7501893|\n",
      "|      3414|RAY SMOCKED CAMI TOP|2020-02-03 00:00:00|            3414|           4782|  0.7671956|\n",
      "+----------+--------------------+-------------------+----------------+---------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cosin_result_path_df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef58e2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv(cosin_result_path3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca544039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df=df.rename(columns={'Product_A':'original_product','Product_B':'matched_product'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19812805",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[['original_product','matched_product','cosin_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "037f9953",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('cosin_similarity_2024-02-28.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d517345a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4349"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Product_df.select('product_id').collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42ef49f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_product</th>\n",
       "      <th>matched_product</th>\n",
       "      <th>cosin_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1292300</th>\n",
       "      <td>6600</td>\n",
       "      <td>1340</td>\n",
       "      <td>0.875849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1292301</th>\n",
       "      <td>6600</td>\n",
       "      <td>8767</td>\n",
       "      <td>0.794721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1292302</th>\n",
       "      <td>6600</td>\n",
       "      <td>8255</td>\n",
       "      <td>0.837213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1292303</th>\n",
       "      <td>6600</td>\n",
       "      <td>3153</td>\n",
       "      <td>0.864809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1292304</th>\n",
       "      <td>6600</td>\n",
       "      <td>7155</td>\n",
       "      <td>0.830372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1292618</th>\n",
       "      <td>6600</td>\n",
       "      <td>3443</td>\n",
       "      <td>0.868823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1292619</th>\n",
       "      <td>6600</td>\n",
       "      <td>1691</td>\n",
       "      <td>0.818983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1292620</th>\n",
       "      <td>6600</td>\n",
       "      <td>3979</td>\n",
       "      <td>0.841117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1292621</th>\n",
       "      <td>6600</td>\n",
       "      <td>3120</td>\n",
       "      <td>0.825599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1292622</th>\n",
       "      <td>6600</td>\n",
       "      <td>6396</td>\n",
       "      <td>0.914567</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>323 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         original_product  matched_product  cosin_score\n",
       "1292300              6600             1340     0.875849\n",
       "1292301              6600             8767     0.794721\n",
       "1292302              6600             8255     0.837213\n",
       "1292303              6600             3153     0.864809\n",
       "1292304              6600             7155     0.830372\n",
       "...                   ...              ...          ...\n",
       "1292618              6600             3443     0.868823\n",
       "1292619              6600             1691     0.818983\n",
       "1292620              6600             3979     0.841117\n",
       "1292621              6600             3120     0.825599\n",
       "1292622              6600             6396     0.914567\n",
       "\n",
       "[323 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['original_product']==6600 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77664592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosin_result_path_df2=cosin_result_path_df2.select(['cosin_score','Product_A','Product_B'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acff1a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosin_result_path_df2=cosin_result_path_df2.select(F.col('cosin_score').alias('cosin_score'),F.col('Product_A').alias('original_product'),F.col('Product_B').alias('matched_product'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0b63b18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4308"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "len(np.unique(cosin_result_path_df3.select('original_product').collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9ba29af",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o58.collectToPython.\n: java.lang.OutOfMemoryError: GC overhead limit exceeded\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1._next(SparkPlan.scala:415)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1.getNext(SparkPlan.scala:426)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1.getNext(SparkPlan.scala:412)\r\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat org.apache.spark.util.NextIterator.foreach(NextIterator.scala:21)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeCollect$1(SparkPlan.scala:449)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeCollect$1$adapted(SparkPlan.scala:448)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$3065/1821783282.apply(Unknown Source)\r\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\r\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:448)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:4160)\r\n\tat org.apache.spark.sql.Dataset$$Lambda$3010/322890663.apply(Unknown Source)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)\r\n\tat org.apache.spark.sql.Dataset$$Lambda$1997/1554730948.apply(Unknown Source)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)\r\n\tat org.apache.spark.sql.Dataset$$Lambda$1651/683631442.apply(Unknown Source)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$1662/1513951348.apply(Unknown Source)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$1652/725740558.apply(Unknown Source)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)\r\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:4157)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m cosin_result_path_df2\u001b[38;5;241m=\u001b[39mcosin_result_path_df2\u001b[38;5;241m.\u001b[39mtoPandas()\n",
      "File \u001b[1;32mc:\\Users\\Z2 mini\\.conda\\envs\\RS\\Lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:208\u001b[0m, in \u001b[0;36mPandasConversionMixin.toPandas\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    205\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;66;03m# Below is toPandas without Arrow optimization.\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m pdf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_records(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollect(), columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[0;32m    209\u001b[0m column_counter \u001b[38;5;241m=\u001b[39m Counter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[0;32m    211\u001b[0m corrected_dtypes: List[Optional[Type]] \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema)\n",
      "File \u001b[1;32mc:\\Users\\Z2 mini\\.conda\\envs\\RS\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:1216\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1196\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[0;32m   1197\u001b[0m \n\u001b[0;32m   1198\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1213\u001b[0m \u001b[38;5;124;03m[Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\u001b[39;00m\n\u001b[0;32m   1214\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1215\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc):\n\u001b[1;32m-> 1216\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mcollectToPython()\n\u001b[0;32m   1217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001b[1;32mc:\\Users\\Z2 mini\\.conda\\envs\\RS\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\Z2 mini\\.conda\\envs\\RS\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\Z2 mini\\.conda\\envs\\RS\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o58.collectToPython.\n: java.lang.OutOfMemoryError: GC overhead limit exceeded\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1._next(SparkPlan.scala:415)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1.getNext(SparkPlan.scala:426)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1.getNext(SparkPlan.scala:412)\r\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat org.apache.spark.util.NextIterator.foreach(NextIterator.scala:21)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeCollect$1(SparkPlan.scala:449)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeCollect$1$adapted(SparkPlan.scala:448)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$3065/1821783282.apply(Unknown Source)\r\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\r\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:448)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:4160)\r\n\tat org.apache.spark.sql.Dataset$$Lambda$3010/322890663.apply(Unknown Source)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)\r\n\tat org.apache.spark.sql.Dataset$$Lambda$1997/1554730948.apply(Unknown Source)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)\r\n\tat org.apache.spark.sql.Dataset$$Lambda$1651/683631442.apply(Unknown Source)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$1662/1513951348.apply(Unknown Source)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$1652/725740558.apply(Unknown Source)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)\r\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:4157)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n"
     ]
    }
   ],
   "source": [
    "# cosin_result_path_df2=cosin_result_path_df2.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5fe90566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosin_result_path_df1=cosin_result_path_df1[['original_product','matched_product','cosin_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "606b5622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_product</th>\n",
       "      <th>matched_product</th>\n",
       "      <th>cosin_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3414</td>\n",
       "      <td>5325</td>\n",
       "      <td>-596.494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3414</td>\n",
       "      <td>7762</td>\n",
       "      <td>-1099.317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3414</td>\n",
       "      <td>7378</td>\n",
       "      <td>-962.506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3414</td>\n",
       "      <td>5316</td>\n",
       "      <td>-598.485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3414</td>\n",
       "      <td>5320</td>\n",
       "      <td>-595.226</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  original_product matched_product cosin_score\n",
       "0             3414            5325    -596.494\n",
       "1             3414            7762   -1099.317\n",
       "2             3414            7378    -962.506\n",
       "3             3414            5316    -598.485\n",
       "4             3414            5320    -595.226"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cosin_result_path_df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d85cb35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b728af1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_scaling(feature_vector):\n",
    "    min_val = min(feature_vector)\n",
    "    max_val = max(feature_vector)\n",
    "    scaled_vector = [round(((x - min_val) / (max_val - min_val)),2) for x in feature_vector]\n",
    "    return scaled_vector\n",
    "\n",
    "def standardize_feature_vector(feature_vector):\n",
    "    mean_val = sum(feature_vector) / len(feature_vector)\n",
    "    std_dev = (sum([(x - mean_val) ** 2 for x in feature_vector]) / len(feature_vector)) ** 0.5\n",
    "    standardized_vector = [(x - mean_val) / std_dev for x in feature_vector]\n",
    "    return standardized_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15fb1ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosin_result_path_df1.to_csv('cosin_similarity.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0aace399",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import datediff,to_date,col,lit\n",
    "def date_difference(Product_df,cosin_date):\n",
    "    cosin_date=Product_df.join(cosin_result_path_df1,cosin_result_path_df1.original_product==Product_df.product_id,'inner')\n",
    "    cosin_date=cosin_date.withColumnRenamed('published_at','reference_date').select(['original_product','matched_product','reference_date','cosin_score'])\n",
    "    cosin_date=cosin_date.join(Product_df,Product_df.product_id==cosin_date.matched_product,'inner').select(['original_product','matched_product','reference_date','cosin_score','published_at'])\n",
    "    cosin_date=cosin_date.withColumn('datediff',datediff(to_date('reference_date'),to_date('published_at')))\n",
    "    cosin_date=cosin_date.withColumn('datediff',datediff(to_date(current_date()),to_date('reference_date')))\n",
    "    date_diff=[ int(d[0]) for d in  cosin_date.select('datediff').collect()]\n",
    "    min_max_vector = min_max_scaling( date_diff)\n",
    "    # standardize_vector=standardize_feature_vector(date_diff)\n",
    "    cosin_date=cosin_date.toPandas()\n",
    "    print('change to pandas')\n",
    "    cosin_date['Min_Max_scalling']=min_max_vector\n",
    "    # cosin_date['standardize_scalling']=standardize_vector\n",
    "    cosin_date['cosin_score_min_max']=cosin_date.Min_Max_scalling*cosin_date.cosin_score.astype(float)\n",
    "    df=df.rename(columns={'cosin_score_min_max':'cosin_score'})\n",
    "    # cosin_date['cosin_score_standization']=cosin_date.standardize_scalling*cosin_date.cosin_score.astype(float)\n",
    "    # cosin_date=cosin_date.withColumn('Min_Max',F.array([F.lit( float(i )) for i in scaled_vector]))\n",
    "    # # cosin_date=cosin_date.withColumn('cosin_score_2',col('cosin_score')*col('datediff'))\n",
    "    # cosin_date=cosin_date.select(['original_product','matched_product','cosin_score_2'])\n",
    "    # cosin_date=cosin_date.withColumnRenamed('cosin_score_2','cosin_score')\n",
    "    # cosin_date=cosin_date.select('*',round('cosin_score',3).alias('cosin_score_1'))\n",
    "    # cosin_date=cosin_date.select(['original_product','matched_product','cosin_score_1'])\n",
    "    # cosin_date=cosin_date.withColumnRenamed('cosin_score_1','cosin_score')\n",
    "    # cosin_date=cosin_date.toPandas()\n",
    "    # cosin_date=cosin_date[['original_product','matched_product','cosin_score']]\n",
    "    return cosin_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fe1901d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o129.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 26.0 failed 1 times, most recent failure: Lost task 0.0 in stage 26.0 (TID 41) (Z2mini executor driver): org.apache.spark.SparkFileNotFoundException: File file:/D:/fp5/fp-new/Code/notebook/cosin_similarity_date_diff_2024-02-26.csv does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$2656/2128503292.apply(Unknown Source)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\r\n\tat org.apache.spark.rdd.RDD$$Lambda$2653/2134162903.apply(Unknown Source)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2609/385827395.apply(Unknown Source)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$Lambda$3401/1903740401.apply(Unknown Source)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$Lambda$3399/1848960472.apply(Unknown Source)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3326)\r\n\tat org.apache.spark.sql.Dataset$$Lambda$1674/1287236694.apply(Unknown Source)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)\r\n\tat org.apache.spark.sql.Dataset$$Lambda$2021/1328912379.apply(Unknown Source)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)\r\n\tat org.apache.spark.sql.Dataset$$Lambda$1675/1358692966.apply(Unknown Source)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$1686/1828016873.apply(Unknown Source)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$1676/1791630199.apply(Unknown Source)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3326)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3549)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkFileNotFoundException: File file:/D:/fp5/fp-new/Code/notebook/cosin_similarity_date_diff_2024-02-26.csv does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$2656/2128503292.apply(Unknown Source)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\r\n\tat org.apache.spark.rdd.RDD$$Lambda$2653/2134162903.apply(Unknown Source)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2609/385827395.apply(Unknown Source)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m cosin_result_path_df3\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32mc:\\Users\\Z2 mini\\.conda\\envs\\RS\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:899\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m    894\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    895\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m    896\u001b[0m     )\n\u001b[0;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[1;32m--> 899\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mshowString(n, \u001b[38;5;241m20\u001b[39m, vertical))\n\u001b[0;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    901\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Z2 mini\\.conda\\envs\\RS\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\Z2 mini\\.conda\\envs\\RS\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\Z2 mini\\.conda\\envs\\RS\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o129.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 26.0 failed 1 times, most recent failure: Lost task 0.0 in stage 26.0 (TID 41) (Z2mini executor driver): org.apache.spark.SparkFileNotFoundException: File file:/D:/fp5/fp-new/Code/notebook/cosin_similarity_date_diff_2024-02-26.csv does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$2656/2128503292.apply(Unknown Source)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\r\n\tat org.apache.spark.rdd.RDD$$Lambda$2653/2134162903.apply(Unknown Source)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2609/385827395.apply(Unknown Source)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$Lambda$3401/1903740401.apply(Unknown Source)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$Lambda$3399/1848960472.apply(Unknown Source)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3326)\r\n\tat org.apache.spark.sql.Dataset$$Lambda$1674/1287236694.apply(Unknown Source)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)\r\n\tat org.apache.spark.sql.Dataset$$Lambda$2021/1328912379.apply(Unknown Source)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)\r\n\tat org.apache.spark.sql.Dataset$$Lambda$1675/1358692966.apply(Unknown Source)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$1686/1828016873.apply(Unknown Source)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$1676/1791630199.apply(Unknown Source)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3326)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3549)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkFileNotFoundException: File file:/D:/fp5/fp-new/Code/notebook/cosin_similarity_date_diff_2024-02-26.csv does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$2656/2128503292.apply(Unknown Source)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\r\n\tat org.apache.spark.rdd.RDD$$Lambda$2653/2134162903.apply(Unknown Source)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2609/385827395.apply(Unknown Source)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "cosin_result_path_df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7fd50e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "change to pandas\n"
     ]
    }
   ],
   "source": [
    "cosin_date=date_difference(Product_df,cosin_result_path_df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5523fd1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4214"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "len(np.unique(cosin_date.original_product.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f5ac0595",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosin_date_min_max=cosin_date[['original_product','matched_product','cosin_score_min_max']]\n",
    "cosin_date_stand=cosin_date[['original_product','matched_product','cosin_score_standization']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "09c223c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosin_date_min_max.to_csv('cosin_similarity_dev_stand.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "58b29f59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4172"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "len(np.unique(cosin_date_min_max['original_product']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "052b6672",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosin_date_stand.to_csv('cosin_similarity_dev_stand.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dce7329f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AddArrayColumnExample\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = [(\"John\", 25), (\"Alice\", 30), (\"Bob\", 35)]\n",
    "columns = [\"Name\", \"Age\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Array of lists\n",
    "array_of_lists = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n",
    "\n",
    "# Add the array as a new column\n",
    "df_with_array_column = df.withColumn(\"ArrayColumn\", lit(array_of_lists))\n",
    "\n",
    "# Show the DataFrame\n",
    "# df_with_array_column.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d236c2d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o129370.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 42.0 failed 1 times, most recent failure: Lost task 0.0 in stage 42.0 (TID 72) (Z2mini executor driver): java.io.IOException: Cannot run program \"python3\": CreateProcess error=2, The system cannot find the file specified\r\n\tat java.lang.ProcessBuilder.start(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:181)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2587/1452640335.apply(Unknown Source)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.io.IOException: CreateProcess error=2, The system cannot find the file specified\r\n\tat java.lang.ProcessImpl.create(Native Method)\r\n\tat java.lang.ProcessImpl.<init>(Unknown Source)\r\n\tat java.lang.ProcessImpl.start(Unknown Source)\r\n\t... 35 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$Lambda$3434/1374122877.apply(Unknown Source)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$Lambda$3432/746127726.apply(Unknown Source)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:4160)\r\n\tat org.apache.spark.sql.Dataset$$Lambda$3306/1360927655.apply(Unknown Source)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)\r\n\tat org.apache.spark.sql.Dataset$$Lambda$1998/737624408.apply(Unknown Source)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)\r\n\tat org.apache.spark.sql.Dataset$$Lambda$1652/1962775268.apply(Unknown Source)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$1663/883575677.apply(Unknown Source)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$1653/1270862622.apply(Unknown Source)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)\r\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:4157)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.io.IOException: Cannot run program \"python3\": CreateProcess error=2, The system cannot find the file specified\r\n\tat java.lang.ProcessBuilder.start(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:181)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2587/1452640335.apply(Unknown Source)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\nCaused by: java.io.IOException: CreateProcess error=2, The system cannot find the file specified\r\n\tat java.lang.ProcessImpl.create(Native Method)\r\n\tat java.lang.ProcessImpl.<init>(Unknown Source)\r\n\tat java.lang.ProcessImpl.start(Unknown Source)\r\n\t... 35 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_with_array_column\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[1;32mc:\\Users\\Z2 mini\\.conda\\envs\\RS\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:2821\u001b[0m, in \u001b[0;36mDataFrame.head\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m   2789\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the first ``n`` rows.\u001b[39;00m\n\u001b[0;32m   2790\u001b[0m \n\u001b[0;32m   2791\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2818\u001b[0m \u001b[38;5;124;03m[Row(age=2, name='Alice')]\u001b[39;00m\n\u001b[0;32m   2819\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2820\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 2821\u001b[0m     rs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m   2822\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m rs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m rs \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2823\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(n)\n",
      "File \u001b[1;32mc:\\Users\\Z2 mini\\.conda\\envs\\RS\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:2823\u001b[0m, in \u001b[0;36mDataFrame.head\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m   2821\u001b[0m     rs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m   2822\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m rs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m rs \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 2823\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(n)\n",
      "File \u001b[1;32mc:\\Users\\Z2 mini\\.conda\\envs\\RS\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:1322\u001b[0m, in \u001b[0;36mDataFrame.take\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m   1293\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtake\u001b[39m(\u001b[38;5;28mself\u001b[39m, num: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Row]:\n\u001b[0;32m   1294\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the first ``num`` rows as a :class:`list` of :class:`Row`.\u001b[39;00m\n\u001b[0;32m   1295\u001b[0m \n\u001b[0;32m   1296\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1320\u001b[0m \u001b[38;5;124;03m    [Row(age=14, name='Tom'), Row(age=23, name='Alice')]\u001b[39;00m\n\u001b[0;32m   1321\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlimit(num)\u001b[38;5;241m.\u001b[39mcollect()\n",
      "File \u001b[1;32mc:\\Users\\Z2 mini\\.conda\\envs\\RS\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:1216\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1196\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[0;32m   1197\u001b[0m \n\u001b[0;32m   1198\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1213\u001b[0m \u001b[38;5;124;03m[Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\u001b[39;00m\n\u001b[0;32m   1214\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1215\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc):\n\u001b[1;32m-> 1216\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mcollectToPython()\n\u001b[0;32m   1217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001b[1;32mc:\\Users\\Z2 mini\\.conda\\envs\\RS\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\Z2 mini\\.conda\\envs\\RS\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\Z2 mini\\.conda\\envs\\RS\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o129370.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 42.0 failed 1 times, most recent failure: Lost task 0.0 in stage 42.0 (TID 72) (Z2mini executor driver): java.io.IOException: Cannot run program \"python3\": CreateProcess error=2, The system cannot find the file specified\r\n\tat java.lang.ProcessBuilder.start(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:181)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2587/1452640335.apply(Unknown Source)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.io.IOException: CreateProcess error=2, The system cannot find the file specified\r\n\tat java.lang.ProcessImpl.create(Native Method)\r\n\tat java.lang.ProcessImpl.<init>(Unknown Source)\r\n\tat java.lang.ProcessImpl.start(Unknown Source)\r\n\t... 35 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$Lambda$3434/1374122877.apply(Unknown Source)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$Lambda$3432/746127726.apply(Unknown Source)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:4160)\r\n\tat org.apache.spark.sql.Dataset$$Lambda$3306/1360927655.apply(Unknown Source)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)\r\n\tat org.apache.spark.sql.Dataset$$Lambda$1998/737624408.apply(Unknown Source)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)\r\n\tat org.apache.spark.sql.Dataset$$Lambda$1652/1962775268.apply(Unknown Source)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$1663/883575677.apply(Unknown Source)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$1653/1270862622.apply(Unknown Source)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)\r\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:4157)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.io.IOException: Cannot run program \"python3\": CreateProcess error=2, The system cannot find the file specified\r\n\tat java.lang.ProcessBuilder.start(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:181)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2587/1452640335.apply(Unknown Source)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\nCaused by: java.io.IOException: CreateProcess error=2, The system cannot find the file specified\r\n\tat java.lang.ProcessImpl.create(Native Method)\r\n\tat java.lang.ProcessImpl.<init>(Unknown Source)\r\n\tat java.lang.ProcessImpl.start(Unknown Source)\r\n\t... 35 more\r\n"
     ]
    }
   ],
   "source": [
    "df_with_array_column.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "13cb260b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[853, 239, 1548, 763, 705, 134, 1700, 983, 902, 1194]\n",
      "[0.38, 0.1, 0.68, 0.34, 0.31, 0.06, 0.75, 0.43, 0.4, 0.53]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Invalid argument, not a string or column: 0.38 of type <class 'float'>. For column literals, use 'lit', 'array', 'struct' or 'create_map' function.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df\u001b[38;5;241m=\u001b[39mdate_difference(Product_df,cosin_result_path_df1)\n",
      "Cell \u001b[1;32mIn[18], line 13\u001b[0m, in \u001b[0;36mdate_difference\u001b[1;34m(Product_df, cosin_date)\u001b[0m\n\u001b[0;32m     11\u001b[0m scaled_vector \u001b[38;5;241m=\u001b[39m min_max_scaling( date_diff)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(scaled_vector[:\u001b[38;5;241m10\u001b[39m])\n\u001b[1;32m---> 13\u001b[0m cosin_date\u001b[38;5;241m=\u001b[39mcosin_date\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMin_Max\u001b[39m\u001b[38;5;124m'\u001b[39m,F\u001b[38;5;241m.\u001b[39marray(scaled_vector))\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# # cosin_date=cosin_date.withColumn('cosin_score_2',col('cosin_score')*col('datediff'))\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# cosin_date=cosin_date.select(['original_product','matched_product','cosin_score_2'])\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# cosin_date=cosin_date.withColumnRenamed('cosin_score_2','cosin_score')\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# cosin_date=cosin_date.toPandas()\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# cosin_date=cosin_date[['original_product','matched_product','cosin_score']]\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cosin_date\n",
      "File \u001b[1;32mc:\\Users\\Z2 mini\\.conda\\envs\\RS\\Lib\\site-packages\\pyspark\\sql\\utils.py:160\u001b[0m, in \u001b[0;36mtry_remote_functions.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(functions, f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Z2 mini\\.conda\\envs\\RS\\Lib\\site-packages\\pyspark\\sql\\functions.py:7285\u001b[0m, in \u001b[0;36marray\u001b[1;34m(*cols)\u001b[0m\n\u001b[0;32m   7283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(cols) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(cols[\u001b[38;5;241m0\u001b[39m], (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mset\u001b[39m)):\n\u001b[0;32m   7284\u001b[0m     cols \u001b[38;5;241m=\u001b[39m cols[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m-> 7285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _invoke_function_over_seq_of_columns(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m, cols)\n",
      "File \u001b[1;32mc:\\Users\\Z2 mini\\.conda\\envs\\RS\\Lib\\site-packages\\pyspark\\sql\\functions.py:112\u001b[0m, in \u001b[0;36m_invoke_function_over_seq_of_columns\u001b[1;34m(name, cols)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;124;03mInvokes unary JVM function identified by name with\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;124;03mand wraps the result with :class:`~pyspark.sql.Column`.\u001b[39;00m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m sc \u001b[38;5;241m=\u001b[39m get_active_spark_context()\n\u001b[1;32m--> 112\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _invoke_function(name, _to_seq(sc, cols, _to_java_column))\n",
      "File \u001b[1;32mc:\\Users\\Z2 mini\\.conda\\envs\\RS\\Lib\\site-packages\\pyspark\\sql\\column.py:86\u001b[0m, in \u001b[0;36m_to_seq\u001b[1;34m(sc, cols, converter)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;124;03mConvert a list of Columns (or names) into a JVM Seq of Column.\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \n\u001b[0;32m     82\u001b[0m \u001b[38;5;124;03mAn optional `converter` could be used to convert items in `cols`\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;124;03minto JVM Column objects.\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m converter:\n\u001b[1;32m---> 86\u001b[0m     cols \u001b[38;5;241m=\u001b[39m [converter(c) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m cols]\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoSeq(cols)\n",
      "File \u001b[1;32mc:\\Users\\Z2 mini\\.conda\\envs\\RS\\Lib\\site-packages\\pyspark\\sql\\column.py:86\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;124;03mConvert a list of Columns (or names) into a JVM Seq of Column.\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \n\u001b[0;32m     82\u001b[0m \u001b[38;5;124;03mAn optional `converter` could be used to convert items in `cols`\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;124;03minto JVM Column objects.\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m converter:\n\u001b[1;32m---> 86\u001b[0m     cols \u001b[38;5;241m=\u001b[39m [converter(c) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m cols]\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoSeq(cols)\n",
      "File \u001b[1;32mc:\\Users\\Z2 mini\\.conda\\envs\\RS\\Lib\\site-packages\\pyspark\\sql\\column.py:65\u001b[0m, in \u001b[0;36m_to_java_column\u001b[1;34m(col)\u001b[0m\n\u001b[0;32m     63\u001b[0m     jcol \u001b[38;5;241m=\u001b[39m _create_column_from_name(col)\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m     66\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid argument, not a string or column: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     67\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m of type \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     68\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor column literals, use \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlit\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstruct\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcreate_map\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     69\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(col, \u001b[38;5;28mtype\u001b[39m(col))\n\u001b[0;32m     70\u001b[0m     )\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m jcol\n",
      "\u001b[1;31mTypeError\u001b[0m: Invalid argument, not a string or column: 0.38 of type <class 'float'>. For column literals, use 'lit', 'array', 'struct' or 'create_map' function."
     ]
    }
   ],
   "source": [
    "df=date_difference(Product_df,cosin_result_path_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7520526e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------------+-------------------+-----------+-------------------+--------+\n",
      "|original_product|matched_product|     reference_date|cosin_score|       published_at|datediff|\n",
      "+----------------+---------------+-------------------+-----------+-------------------+--------+\n",
      "|            4821|           4894|2021-10-04 18:56:21|      0.979|2021-10-21 12:56:20|     852|\n",
      "|            4821|           7650|2021-10-04 18:56:21|      0.914|2023-06-27 08:10:35|     238|\n",
      "|            4821|           3266|2021-10-04 18:56:21|      0.974|2019-11-26 00:00:00|    1547|\n",
      "|            4821|           5234|2021-10-04 18:56:21|       0.97|2022-01-19 10:38:35|     762|\n",
      "|            4821|           5408|2021-10-04 18:56:21|      0.902|2022-03-18 09:54:57|     704|\n",
      "|            4821|           8279|2021-10-04 18:56:21|      0.952|2023-10-10 07:27:36|     133|\n",
      "|            4821|           2644|2021-10-04 18:56:21|      0.846|2019-06-27 21:05:20|    1699|\n",
      "|            4821|           4417|2021-10-04 18:56:21|      0.869|2021-06-13 12:18:17|     982|\n",
      "|            4821|           4690|2021-10-04 18:56:21|       0.92|2021-09-02 09:24:07|     901|\n",
      "|            4821|           3848|2021-10-04 18:56:21|       0.81|2020-11-14 00:00:00|    1193|\n",
      "|            4821|           5543|2021-10-04 18:56:21|      0.865|2022-05-04 10:52:45|     657|\n",
      "|            4821|           4980|2021-10-04 18:56:21|      0.822|2022-01-20 08:50:37|     761|\n",
      "|            4821|           4934|2021-10-04 18:56:21|      0.807|2021-11-23 09:08:56|     819|\n",
      "|            4821|           7539|2021-10-04 18:56:21|      0.964|2023-07-02 08:01:34|     233|\n",
      "|            4821|           7689|2021-10-04 18:56:21|      0.965|2023-07-03 08:20:48|     232|\n",
      "|            4821|           2605|2021-10-04 18:56:21|      0.921|2019-06-18 00:00:00|    1708|\n",
      "|            4821|           5412|2021-10-04 18:56:21|      0.955|2022-03-24 12:53:52|     698|\n",
      "|            4821|           3570|2021-10-04 18:56:21|      0.953|2020-02-25 00:00:00|    1456|\n",
      "|            4821|           6423|2021-10-04 18:56:21|      0.808|2022-10-26 22:01:29|     482|\n",
      "|            4821|           5352|2021-10-04 18:56:21|      0.901|2022-04-11 14:42:00|     680|\n",
      "+----------------+---------------+-------------------+-----------+-------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8392cdb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7567a557",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_diff=[ d[0] for d in  df.select('datediff').collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a6b159bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original feature vector: [125, 266, 973, 168, 1194, 1142, 890, 714, 614, 418, 669, 963, 239, 78, 25, 655, 71, 1364, 156, 246, 600, 232, 859, 1657, 1721, 1007, 218, 281, 118, 238, 746, 96, 1049, 931, 818, 218, 589, 827, 458, 71, 881, 777, 104, 963, 34, 699, 727, 8, 414, 232, 371, 1008, 925, 1532, 441, 852, 837, 181, 630, 99, 238, 104, 966, 803, 266, 869, 980, 230, 146, 806, 614, 599, 993, 99, 1660, 991, 265, 643, 266, 953, 238, 20, 1318, 237, 405, 1531, 741, 1933, 134, 118, 217, 28, 1016, 960, 819, 461, 1264, 232, 330, 239, 223, 1223, 1075, 1548, 995, 1719, 861, 838, 1203, 238, 750, 645, 1356, 1539, 880, 880, 1469, 486, 418, 330, 173, 1016, 71, 813, 733, 54, 1567, 109, 819, 133, 70, 887, 203, 334, 911, 432, 1463, 1610, 188, 802, 812, 1030, 819, 379, 749, 71, 71, 959, 1314, 872, 146, 382, 155, 623, 146, 911, 963, 1594, 223, 764, 1594, 707, 201, 658, 139, 266, 1057, 218, 505, 1067, 953, 512, 140, 1083, 508, 862, 405, 701, 1077, 1046, 690, 693, 141, 894, 145, 508, 1547, 883, 278, 971, 451, 482, 224, 1020, 215, 887, 959, 1541, 286, 1065]\n",
      "Scaled feature vector: [0.05, 0.12, 0.43, 0.07, 0.53, 0.5, 0.39, 0.31, 0.27, 0.18, 0.29, 0.42, 0.1, 0.03, 0.01, 0.29, 0.03, 0.6, 0.07, 0.11, 0.26, 0.1, 0.38, 0.73, 0.76, 0.44, 0.1, 0.12, 0.05, 0.1, 0.33, 0.04, 0.46, 0.41, 0.36, 0.1, 0.26, 0.36, 0.2, 0.03, 0.39, 0.34, 0.05, 0.42, 0.01, 0.31, 0.32, 0.0, 0.18, 0.1, 0.16, 0.44, 0.41, 0.67, 0.19, 0.38, 0.37, 0.08, 0.28, 0.04, 0.1, 0.05, 0.43, 0.35, 0.12, 0.38, 0.43, 0.1, 0.06, 0.35, 0.27, 0.26, 0.44, 0.04, 0.73, 0.44, 0.12, 0.28, 0.12, 0.42, 0.1, 0.01, 0.58, 0.1, 0.18, 0.67, 0.33, 0.85, 0.06, 0.05, 0.1, 0.01, 0.45, 0.42, 0.36, 0.2, 0.56, 0.1, 0.14, 0.1, 0.1, 0.54, 0.47, 0.68, 0.44, 0.76, 0.38, 0.37, 0.53, 0.1, 0.33, 0.28, 0.6, 0.68, 0.39, 0.39, 0.65, 0.21, 0.18, 0.14, 0.08, 0.45, 0.03, 0.36, 0.32, 0.02, 0.69, 0.05, 0.36, 0.06, 0.03, 0.39, 0.09, 0.15, 0.4, 0.19, 0.64, 0.71, 0.08, 0.35, 0.36, 0.45, 0.36, 0.17, 0.33, 0.03, 0.03, 0.42, 0.58, 0.38, 0.06, 0.17, 0.07, 0.27, 0.06, 0.4, 0.42, 0.7, 0.1, 0.34, 0.7, 0.31, 0.09, 0.29, 0.06, 0.12, 0.47, 0.1, 0.22, 0.47, 0.42, 0.23, 0.06, 0.48, 0.22, 0.38, 0.18, 0.31, 0.47, 0.46, 0.3, 0.3, 0.06, 0.39, 0.06, 0.22, 0.68, 0.39, 0.12, 0.43, 0.2, 0.21, 0.1, 0.45, 0.09, 0.39, 0.42, 0.68, 0.13, 0.47]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage:\n",
    "scaled_vector = min_max_scaling( date_diff)\n",
    "print(\"Original feature vector:\", date_diff[200:400])\n",
    "print(\"Scaled feature vector:\", scaled_vector[200:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "405b7e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original feature vector: [125, 266, 973, 168, 1194, 1142, 890, 714, 614, 418, 669, 963, 239, 78, 25, 655, 71, 1364, 156, 246, 600, 232, 859, 1657, 1721, 1007, 218, 281, 118, 238, 746, 96, 1049, 931, 818, 218, 589, 827, 458, 71, 881, 777, 104, 963, 34, 699, 727, 8, 414, 232, 371, 1008, 925, 1532, 441, 852, 837, 181, 630, 99, 238, 104, 966, 803, 266, 869, 980, 230, 146, 806, 614, 599, 993, 99, 1660, 991, 265, 643, 266, 953, 238, 20, 1318, 237, 405, 1531, 741, 1933, 134, 118, 217, 28, 1016, 960, 819, 461, 1264, 232, 330, 239, 223, 1223, 1075, 1548, 995, 1719, 861, 838, 1203, 238, 750, 645, 1356, 1539, 880, 880, 1469, 486, 418, 330, 173, 1016, 71, 813, 733, 54, 1567, 109, 819, 133, 70, 887, 203, 334, 911, 432, 1463, 1610, 188, 802, 812, 1030, 819, 379, 749, 71, 71, 959, 1314, 872, 146, 382, 155, 623, 146, 911, 963, 1594, 223, 764, 1594, 707, 201, 658, 139, 266, 1057, 218, 505, 1067, 953, 512, 140, 1083, 508, 862, 405, 701, 1077, 1046, 690, 693, 141, 894, 145, 508, 1547, 883, 278, 971, 451, 482, 224, 1020, 215, 887, 959, 1541, 286, 1065]\n",
      "Standardized feature vector: [-1.1125686930754732, -0.7661428113395597, 0.9709004396341341, -1.0069210837517548, 1.513880013135105, 1.3861201134878176, 0.7669759844278871, 0.33455786254476105, 0.08886574783843947, -0.3926907969859509, 0.22399641092691636, 0.9463312281635019, -0.8324796823102666, -1.2280439869874444, -1.3582608077817948, 0.1895995148680313, -1.245242435016887, 1.9315566081358515, -1.0364041375165136, -0.8152812342808241, 0.054468851779554434, -0.8496781303397091, 0.6908114288689274, 2.651434504225374, 2.80867745763742, 1.0544357586342834, -0.8840750263985941, -0.7292889941336115, -1.1297671411049157, -0.8349366034573298, 0.413179339250784, -1.1838194063403065, 1.1576264468109385, 0.867709751457479, 0.5900776618393355, -0.8840750263985941, 0.027442719161859058, 0.6121899521629045, -0.2944139511034223, -1.245242435016887, 0.7448636941043182, 0.48934389480974366, -1.1641640371638007, 0.9463312281635019, -1.336148517458226, 0.29770404533881284, 0.3664978374565829, -1.4000284672818695, -0.4025184815742038, -0.8496781303397091, -0.5081660908979221, 1.0568926797813467, 0.8529682245750997, 2.344319360842472, -0.33618161060349694, 0.6736129808394848, 0.6367591636335367, -0.9749811088399332, 0.12817648619145092, -1.1764486428991168, -0.8349366034573298, -1.1641640371638007, 0.9537019916046915, 0.5532238446333873, -0.7661428113395597, 0.7153806403395596, 0.9880988876635766, -0.8545919726338356, -1.0609733489871456, 0.560594608074577, 0.08886574783843947, 0.05201193063249122, 1.0200388625753984, -1.1764486428991168, 2.6588052676665637, 1.0151250202812718, -0.7685997324866229, 0.16011646110327274, -0.7661428113395597, 0.9217620166928697, -0.8349366034573298, -1.370545413517111, 1.8185382353709436, -0.837393524604393, -0.4246307718977727, 2.3418624396954084, 0.4008947335154679, 3.3295447408148213, -1.0904564027519044, -1.1297671411049157, -0.8865319475456573, -1.3508900443406051, 1.0765480489578523, 0.9389604647223122, 0.5925345829863987, -0.2870431876622326, 1.68586449342953, -0.8496781303397091, -0.6088998579275139, -0.8324796823102666, -0.871790420663278, 1.5851307263999381, 1.221506396634582, 2.3836300991954835, 1.0249527048695248, 2.803763615343293, 0.6957252711630538, 0.6392160847805999, 1.5359923034586738, -0.8349366034573298, 0.4230070238390369, 0.16503030339739916, 1.9119012389593457, 2.3615178088719144, 0.742406772957255, 0.742406772957255, 2.189533328577489, -0.2256201589856522, -0.3926907969859509, -0.6088998579275139, -0.9946364780164388, 1.0765480489578523, -1.245242435016887, 0.5777930561040194, 0.38123936433896216, -1.2870100945169616, 2.4303116009896844, -1.1518794314284846, 0.5925345829863987, -1.0929133238989674, -1.24769935616395, 0.7596052209866975, -0.9209288436045424, -0.5990721733392611, 0.8185713285162146, -0.35829390092706587, 2.1747918016951098, 2.5359592103134028, -0.9577826608104906, 0.5507669234863241, 0.5753361349569562, 1.1109449450167375, 0.5925345829863987, -0.4885107217214163, 0.4205501026919736, -1.245242435016887, -1.245242435016887, 0.936503543575249, 1.8087105507826908, 0.7227514037807492, -1.0609733489871456, -0.48113995828022665, -1.0388610586635767, 0.11097803816200841, -1.0609733489871456, 0.8185713285162146, 0.9463312281635019, 2.496648471960391, -0.871790420663278, 0.4574039198979219, 2.496648471960391, 0.31735941451531857, -0.9258426858986688, 0.19697027830922098, -1.0781717970165883, -0.7661428113395597, 1.1772818159874443, -0.8840750263985941, -0.1789386571914511, 1.2018510274580763, 0.9217620166928697, -0.16174020916200857, -1.075714875869525, 1.2411617658110878, -0.17156789375026144, 0.698182192310117, -0.4246307718977727, 0.3026178876329393, 1.2264202389287084, 1.1502556833697488, 0.2755917550152439, 0.28296251845643355, -1.0732579547224617, 0.77680366901614, -1.0634302701342089, -0.17156789375026144, 2.38117317804842, 0.7497775363984446, -0.7366597575748012, 0.9659865973400076, -0.31161239913286476, -0.23544784357390505, -0.8693334995162149, 1.0863757335461053, -0.8914457898397837, 0.7596052209866975, 0.936503543575249, 2.3664316511660406, -0.7170043883982954, 1.1969371851639499]\n"
     ]
    }
   ],
   "source": [
    "def standardize_feature_vector(feature_vector):\n",
    "    mean_val = sum(feature_vector) / len(feature_vector)\n",
    "    std_dev = (sum([(x - mean_val) ** 2 for x in feature_vector]) / len(feature_vector)) ** 0.5\n",
    "    standardized_vector = [(x - mean_val) / std_dev for x in feature_vector]\n",
    "    return standardized_vector\n",
    "\n",
    "# Sample feature vector\n",
    "feature_vector = [-1, -2, -3, 4, 5]\n",
    "\n",
    "# Perform standardization\n",
    "standardized_feature_vector = standardize_feature_vector(date_diff)\n",
    "\n",
    "print(\"Original feature vector:\", date_diff[200:400])\n",
    "print(\"Standardized feature vector:\", standardized_feature_vector[200:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b31b5b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosin_date=cosin_date.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f1e74d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosin_date.to_csv('cosin_similarity.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b4eaed52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(np.unique(cosin_date['original_product'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "de30b49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosin_date.to_csv('cosin_similarity_data_diff.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "afc9f64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosin_result_path_df4=cosin_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "68307a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------------+-----------+\n",
      "|original_product|matched_product|cosin_score|\n",
      "+----------------+---------------+-----------+\n",
      "|            4535|           8666|     -88.74|\n",
      "|            4535|           8655|    -70.602|\n",
      "|            4535|           8624|    -54.351|\n",
      "|            4535|           8163|    -49.952|\n",
      "|            4535|           8545|    -40.896|\n",
      "|            4535|           8587|    -40.044|\n",
      "|            4535|           8588|    -40.044|\n",
      "|            4535|           8541|     -38.28|\n",
      "|            4535|           8162|    -36.636|\n",
      "|            4535|           8369|    -35.862|\n",
      "|            4535|           8434|    -34.788|\n",
      "|            4535|           6698|    -20.982|\n",
      "|            4535|           8335|    -18.656|\n",
      "|            4535|           8093|      2.556|\n",
      "|            4535|           8207|      6.672|\n",
      "|            4535|           7928|     14.824|\n",
      "|            4535|           8094|      18.27|\n",
      "|            4535|           7895|     18.963|\n",
      "|            4535|           7503|     19.803|\n",
      "|            4535|           7917|      20.01|\n",
      "+----------------+---------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "product_title='RIBCAGE SHORT'\n",
    "selected_product_id=Product_df.filter(Product_df['product_title']==product_title).select('product_id').collect()[0][0]\n",
    "recomendation_df_product=cosin_result_path_df4[cosin_result_path_df4['original_product']==selected_product_id]\n",
    "recomendation_df_product=recomendation_df_product.sort(col('cosin_score').asc())\n",
    "recomendation_df_product.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "f2d0dbbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5174\n",
      "8628,8510,8441,8612,8452,8381,8399,8065,8382,8458,8327,8488,8281,8294,7859,7840,8384,7973,8373,8249,8340,7951,8146,8445,8459,7858,8210,7857,7374,7330,7127,6674,6706,6671,6702,6058,6404,6365,6235,5594,5409,5147,5153,5108,5176\n"
     ]
    }
   ],
   "source": [
    "link=[]\n",
    "print(selected_product_id)\n",
    "for row in recomendation_df_product.collect():\n",
    "    if float(row['cosin_score']) < 1 :\n",
    "        id=row['matched_product']\n",
    "        # datediff=row['datediff']\n",
    "        # score=row['cosin_score']\n",
    "        # link.append(int(id))\n",
    "        # product_thumbnail=Product_df.filter(Product_df.product_id==id).select('product_thumbnail').collect()\n",
    "        try:\n",
    "            # print('https://fashionpass.s3.us-west-1.amazonaws.com/products/'+(product_thumbnail[0][0]))\n",
    "            link.append(id)\n",
    "        except:\n",
    "            print('valid string')    \n",
    "print(','.join(link))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3d842d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3939,4407,7137,8167,2668,6832,8202,4815,8245,7534,8570,2006,5441,4286,7811,5091,2459,7998,5194,5943,8477,2591,5602,5032,6224,6845,8552,7641,3962,8081,1976,4627,2687,5860,3963,2844,8331,3323,7141,3623,8476,4811,4900,7733,8753,4946,7960,4285,8097,7804,7726,5172,7844,7803,1951,4331,4311,7725,3769,4810,1231,2328,7308,5940,8143,5382,8250,7661,6798,5410,5938,6283,8080,3961,6223,6678,2177,7826,4962,4108,4406,2254,8051,8637,6890,3192,7148,3217,5423,7602,8752,5381,8550,8750,3564,8664,4927,7997,5438,4143,8254,7535,4625,5736,180,4102,1531,316,6432,1450,5201,5719,5741,6975,6231,7619,5939,8316,8076,2549,8075,3759,5424,5422,5368,3878,5405,5288,5611,859,6225,6073,3440,1952,3423,5404,5369,5182,4963,7140,7986,2332,4978,7876,4210,7800,6540,4287,3205,3575,7802,7796,1949,5279,1614,7938,5089,8754,3385,557,8475,2001,915,6421,5858,6610,3749,3725,5281,8487,5126,5171,4145,2399,7172,7660,2707,5185,7943,8751,2114,2029,5131,6265,278,6115,3396,8054,8867,8400,4409,3422,8068,7124,8523,6397,6880,8660,1251\n"
     ]
    }
   ],
   "source": [
    "selected_product_id=Product_df.filter(Product_df['product_title']==product_title).select('product_id').collect()[0][0]\n",
    "recomendation_df_product=cosin_result_path_df2[cosin_result_path_df2['Product_A']==selected_product_id]\n",
    "link=[]\n",
    "for row in recomendation_df_product.collect():\n",
    "    if float(row['cosin_score'])>=.80:\n",
    "        id=row['Product_B']\n",
    "        # link.append(int(id))\n",
    "        # product_thumbnail=Product_df.filter(Product_df.product_id==id).select('product_thumbnail').collect()\n",
    "        try:\n",
    "            # print('https://fashionpass.s3.us-west-1.amazonaws.com/products/'+(product_thumbnail[0][0]))\n",
    "             link.append(id)\n",
    "        except:\n",
    "            print('valid string')    \n",
    "print(','.join(link))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5356ac60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5444,2324,2940,7779,1668,7630,1200,2747,604,104,3599,978,3713,5342,7675,5973,3921,1557,7555,7874,8112,4763,1790,645,5506,1237,1301,1105,1078,8033,3473,7253,3538,1348,1201,8405,1028,7599,1714,1068,5446,7748,1206,1523,7276,5749,8403,1558,7870,8155,4188,7557,7608,281,461,741,1080,350,114,2897,540,759,8192,4958,1597,8032,7965,1768,3598,8102,490,24,7558,757,3356,8141,7744,5391,1778,289,8156,5448,7443,5447,8132,7254,1848,2924,8231,776,3596,3675,5670,8404,7783,3514,49,1661,4425,7610,7265,1242,2320,4181,4254,3710,7871,1233,1029,4251,1553,7206,4486,1350,1273,66,8191,8232,760,2730,8230,8401,5389,8506,7216,1699,6718,1781,2704,3594,5390,7264,8111,2746,3601,2732,5953,4316,145,2076,8188,6137,881,1779,2317,119,8128,8190,1329,8189\n"
     ]
    }
   ],
   "source": [
    "selected_product_id=Product_df.filter(Product_df['product_title']==product_title).select('product_id').collect()[0][0]\n",
    "recomendation_df_product=cosin_result_path_df3[cosin_result_path_df3['Product_A']==selected_product_id]\n",
    "link=[]\n",
    "for row in recomendation_df_product.collect():\n",
    "    if float(row['cosin_score'])>=.80:\n",
    "        id=row['Product_B']\n",
    "        # link.append(int(id))\n",
    "        # product_thumbnail=Product_df.filter(Product_df.product_id==id).select('product_thumbnail').collect()\n",
    "        try:\n",
    "            # print('https://fashionpass.s3.us-west-1.amazonaws.com/products/'+(product_thumbnail[0][0]))\n",
    "             link.append(id)\n",
    "        except:\n",
    "            print('valid string')    \n",
    "print(','.join(link))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3baf6e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "cs1=pd.read_csv(cosin_result_path3)#configur.get('cosin_similarity_base_path','cs1'))#load_csv(spark_session,file_path=configur.get('cosin_similarity_base_path','cs1'))\n",
    "cs2=pd.read_csv(cosin_result_path2)#configur.get('cosin_similarity_base_path','cs2'))#load_csv(spark=spark_session,file_path=configur.get('cosin_similarity_base_path','cs2'))\n",
    "cs3=pd.read_csv(cosin_result_path1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b6031e7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'6878'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_title='X CAMELIA FARHOODI THE LEILA CHOKER'\n",
    "selected_product_id=[Product_df.filter(Product_df['product_title']==product_title).select('product_id').collect()[0][0]]\n",
    "(selected_product_id[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fbad1351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of product 1\n",
      "7955\n",
      "top re []\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# from Code.utils.libraries import * \n",
    "# from Code.data.dataprocessing.load.load import *\n",
    "# from Code.data.dataprocessing.transform.transform import *\n",
    "# from sessions import *\n",
    "# from pyspark.sql.functions import date_format\n",
    "# from Code.models.model_training.model import *\n",
    "\n",
    "\n",
    "# BASE_PATH=os.getenv('BASEPATH')\n",
    "# CONN_URL = os.getenv('CONN_URL')\n",
    "# PASSWORD = os.getenv('PASSWORD')\n",
    "# USERNAME=os.getenv('USER_NAME')\n",
    "# configur = ConfigParser() \n",
    "# configur.read(BASE_PATH+'config.ini')\n",
    "#spark_session=session(configur.get('driver_path_pyspark','pyspark'))\n",
    "#configur.get('cosin_similarity_base_path','cs3'))#Sload_csv(spark=spark_session,file_path=configur.get('cosin_similarity_base_path','cs3'))\n",
    "\n",
    "cosin_df=[]\n",
    "product_title='SIENA TOP'\n",
    "product_ids=[Product_df.filter(Product_df['product_title']==product_title).select('product_id').collect()[0][0]]#[(Product_df.filter(Product_df['product_title']==product_title).select('product_id').collect()[0][0])]#cs1.Product_A.unique()#[id[0] for id in cs1.select('Product_A').distinct().collect()]  in pyspark [8692,2178,8687,1087,3069]\n",
    "print(\"total number of product\",len(product_ids))\n",
    "# st = time.time()\n",
    "\n",
    "for r in product_ids:\n",
    "    r=int(r)\n",
    "    print(r)\n",
    "    cs1_product_b=[]\n",
    "    cs2_product_b=[]\n",
    "    cs3_product_b=[]\n",
    "\n",
    "    cs1_product_b=cs1[(cs1['Product_A']==r) & (cs1['cosin_score']>=.80)]['Product_B'].unique().tolist()  #[ product[0] for product in cs1.filter((cs1['Product_A']==r) & (cs1['cosin_score']>=.80))['Product_B'].unique()]\n",
    "    cs2_product_b=cs2[(cs2['Product_A']==r) & (cs2['cosin_score']>=.85)]['Product_B'].unique().tolist()#values\n",
    "    cs3_product_b=cs3[(cs3['Product_A']==r) & (cs3['cosin_score']>=.90)]['Product_B'].unique().tolist()#values\n",
    "    # cs2temp=cs2.filter((cs2['Product_A']==r) & (cs2['cosin_score']>=.85))\n",
    "    # cs3temp=cs3.filter((cs3['Product_A']==r) & (cs3['cosin_score']>=.90))\n",
    "    # print(cs1_product_b)\n",
    "    # print(cs2_product_b)\n",
    "    # print(cs3_product_b)\n",
    "\n",
    "    # for row in  cs1temp.collect():\n",
    "    #     if float(row['cosin_score'])>=.80:\n",
    "    #         cs1_product_b.append(row['Product_B'])\n",
    "        \n",
    "    # for row2 in cs2temp.collect():\n",
    "    #     if float(row2['cosin_score'])>=.85:\n",
    "    #         cs1_product_b.append(row2['Product_B'])\n",
    "    #         # cs2_product_b.append(row2['Product_B'])\n",
    "    # for row3 in cs3temp.collect():\n",
    "    #     if float(row3['cosin_score'])>=.90 :\n",
    "    #         cs1_product_b.append(row3['Product_B'])\n",
    "                # cs3_product_b.append(row3['Product_B'])\n",
    "\n",
    "#print(cs1_product_b,cs2_product_b,cs3_product_b)\n",
    "    for id in cs2_product_b:\n",
    "        cs1_product_b.append(id)\n",
    "    for id in cs3_product_b:\n",
    "        cs1_product_b.append(id)\n",
    "    # final_list=cs1_product_b+cs2_product_b+cs3_product_b\n",
    "    my_dict = {i:cs1_product_b.count(i) for i in cs1_product_b}\n",
    "    # print(my_dict)\n",
    "    top_recomendation=[]\n",
    "    for key,value in my_dict.items():\n",
    "        if value >=3:\n",
    "            top_recomendation.append(key)\n",
    "    print('top re',top_recomendation)\n",
    "    cosin_df.append(cs1[(cs1['Product_A']==r) & (cs1['Product_B'].isin(top_recomendation))])\n",
    "    # cosin_df.append(cs1.filter((cs1['Product_A']==r) & (cs1['Product_B'].isin(top_recomendation))))  # in pandas\n",
    "\n",
    "df=cosin_df[0]\n",
    "print(len(cosin_df[1:]))\n",
    "for i in cosin_df[1:]:\n",
    "    #  try:\n",
    "         #df=df.union(i) #in pyspark\n",
    "         df=df._append(i,ignore_index = True) #in pandas\n",
    "    #  except:\n",
    "    #      print('out of index')\n",
    "#df.toPandas().to_csv('cosin_simmilarity1.csv')\n",
    "# et = time.time()\n",
    "# print(f'Execution time:{et-st} seconds')\n",
    "# df.to_csv('cosin_simmilarity2.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
