{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6bbb4369",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark import SparkConf,sql\n",
    "from pyspark.sql.session import SparkSession\n",
    "import findspark\n",
    "import pymysql\n",
    "import pymysql.cursors\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e894d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymysql\n",
      "  Downloading PyMySQL-1.1.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Downloading PyMySQL-1.1.0-py3-none-any.whl (44 kB)\n",
      "   ---------------------------------------- 0.0/44.8 kB ? eta -:--:--\n",
      "   --------- ------------------------------ 10.2/44.8 kB ? eta -:--:--\n",
      "   ------------------ --------------------- 20.5/44.8 kB 330.3 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 41.0/44.8 kB 326.8 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 44.8/44.8 kB 276.2 kB/s eta 0:00:00\n",
      "Installing collected packages: pymysql\n",
      "Successfully installed pymysql-1.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pymysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c80c2f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_path='C:/Users/Bionic Computer/Downloads/ubaidnizami-fp-ai/Zubaidnizami-fp-ai-081beb29d155/Code/Database/product.csv'\n",
    "cosin_accessories='C:/Users/Bionic Computer/Downloads/ubaidnizami-fp-ai/ubaidnizami-fp-ai-081beb29d155/Code/cosin_similarity_registry/tag_collection.csv'\n",
    "cosin_clothing='C:/Users/Bionic Computer/Downloads/ubaidnizami-fp-ai/ubaidnizami-fp-ai-081beb29d155/Code/cosin_similarity_registry/collection_tag.csv'#'C:/Users/Bionic Computer/Downloads/ubaidnizami-fp-ai/ubaidnizami-fp-ai-081beb29d155/cosin_simmilarity1.csv'\n",
    "cosin_result_path3='C:/Users/Bionic Computer/Downloads/ubaidnizami-fp-ai/ubaidnizami-fp-ai-081beb29d155/Code/cosin_similarity_registry/collection_tag_FE.csv'\n",
    "\n",
    "collection_category='C:/Users/Bionic Computer/Downloads/ubaidnizami-fp-ai/ubaidnizami-fp-ai-081beb29d155/Code/Database/collection_category.csv'\n",
    "collection_category_tag='C:/Users/Bionic Computer/Downloads/ubaidnizami-fp-ai/ubaidnizami-fp-ai-081beb29d155/Code/Database/collection_category_tag.csv'\n",
    "product_tag='C:/Users/Bionic Computer/Downloads/ubaidnizami-fp-ai/ubaidnizami-fp-ai-081beb29d155/Code/Database/product_tag.csv'\n",
    "product='C:/Users/Bionic Computer/Downloads/ubaidnizami-fp-ai/ubaidnizami-fp-ai-081beb29d155/Code/Database/product.csv'\n",
    "tag_cloud='C:/Users/Bionic Computer/Downloads/ubaidnizami-fp-ai/ubaidnizami-fp-ai-081beb29d155/Code/Database/tag_cloud.csv'\n",
    "feature_accesories='C:/Users/Bionic Computer/Downloads/ubaidnizami-fp-ai/ubaidnizami-fp-ai-081beb29d155/Code/feature_registry/featurelist_22_1_2024_accessories.csv'\n",
    "feature_clothing='C:/Users/Bionic Computer/Downloads/ubaidnizami-fp-ai/ubaidnizami-fp-ai-081beb29d155/Code/feature_registry/featurelist_12_1_2024_clothing.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d856acc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark MySQL Example\") \\\n",
    "    .config(\"spark.driver.extraClassPath\", \"C:/spark/spark-3.5.0-bin-hadoop3/jars/mysql-connector-java-5.1.46.jar\") \\\n",
    "    .config(\"spark.driver.memory\", \"15g\") \\\n",
    "    .getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3f45dc2e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/D:/PROJECTS/Recomendation_System_FP/Code/Database/product.csv.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m Product_df\u001b[38;5;241m=\u001b[39m\u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcsv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mheader\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproduct_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m cosin_accessories_df\u001b[38;5;241m=\u001b[39mspark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcsv\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheader\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;28;01mTrue\u001b[39;00m) \\\n\u001b[0;32m      4\u001b[0m \u001b[38;5;241m.\u001b[39mload(cosin_accessories)\n\u001b[0;32m      5\u001b[0m cosin_clothing_df\u001b[38;5;241m=\u001b[39mspark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcsv\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheader\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;28;01mTrue\u001b[39;00m) \\\n\u001b[0;32m      6\u001b[0m \u001b[38;5;241m.\u001b[39mload(cosin_clothing)\n",
      "File \u001b[1;32mc:\\Users\\Bionic Computer\\anaconda3\\envs\\RS\\Lib\\site-packages\\pyspark\\sql\\readwriter.py:300\u001b[0m, in \u001b[0;36mDataFrameReader.load\u001b[1;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[0;32m    299\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 300\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    301\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Bionic Computer\\anaconda3\\envs\\RS\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\Bionic Computer\\anaconda3\\envs\\RS\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    171\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/D:/PROJECTS/Recomendation_System_FP/Code/Database/product.csv."
     ]
    }
   ],
   "source": [
    "Product_df=spark.read.format('csv').option('header',True) \\\n",
    ".load(product_path)\n",
    "cosin_accessories_df=spark.read.format('csv').option('header',True) \\\n",
    ".load(cosin_accessories)\n",
    "cosin_clothing_df=spark.read.format('csv').option('header',True) \\\n",
    ".load(cosin_clothing)\n",
    "cosin_result_path_df3=spark.read.format('csv').option('header',True) \\\n",
    ".load(cosin_result_path3)\n",
    "collection_category_df=spark.read.format('csv').option('header',True) \\\n",
    ".load(collection_category)\n",
    "collection_category_tag_df=spark.read.format('csv').option('header',True) \\\n",
    ".load(collection_category_tag)\n",
    "tag_cloud_df=spark.read.format('csv').option('header',True) \\\n",
    ".load(tag_cloud)\n",
    "product_tag_df=spark.read.format('csv').option('header',True) \\\n",
    ".load(product_tag)\n",
    "\n",
    "\n",
    "feature_accesories_df=spark.read.format('csv').option('header',True) \\\n",
    ".load(feature_accesories)\n",
    "feature_clothing_df=spark.read.format('csv').option('header',True) \\\n",
    ".load(feature_clothing)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d33a661",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def feature_list(df=None):\n",
    "    \"\"\"\n",
    "    Extract feature lists from the input DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pyspark.sql.DataFrame): Input DataFrame containing feature columns.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary containing different feature lists.\n",
    "\n",
    "    Feature Lists:\n",
    "    - 'category_collection_FE': List of values extracted from the 'category_collection_FE' column.\n",
    "    - 'categoey_collection_tag_L': List of values extracted from the 'category_collection_tag_L' column.\n",
    "    - 'tag_cloud': List of values extracted from the 'tag_cloud' column.\n",
    "    \"\"\"\n",
    "    # Extract values from 'category_collection_FE', 'category_collection_tag_L', and 'tag_cloud' columns\n",
    "    category_collection_FE = [i[0] for i in df.select('category_collection_FE').collect()]\n",
    "    category_collection_tag_L = [i[0] for i in df.select('category_collection_tag_L').collect()]\n",
    "    tag_cloud = [i[0] for i in df.select('tag_cloud').collect()]\n",
    "\n",
    "    # Return a dictionary containing the extracted feature lists\n",
    "    return {\n",
    "        'category_collection_FE': category_collection_FE,\n",
    "        'categoey_collection_tag_L': category_collection_tag_L,\n",
    "        'tag_cloud': tag_cloud\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c0836db",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_accesories_df=feature_list(feature_accesories_df)\n",
    "feature_clothing_df=feature_list(feature_clothing_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99106160",
   "metadata": {},
   "outputs": [],
   "source": [
    "def duplicate_col(data=None):\n",
    "  \"\"\"\n",
    "    Duplicate columns in a DataFrame by appending '_duplicate_' and the index to the column name.\n",
    "\n",
    "    Parameters:\n",
    "    - data (pd.DataFrame): The input DataFrame with columns to be duplicated.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A new DataFrame with duplicated columns.\n",
    "    \"\"\"\n",
    "  df_cols = data.columns\n",
    "  duplicate_col_index = [idx for idx,\n",
    "  val in enumerate(df_cols) if val in df_cols[:idx]]\n",
    "  for i in duplicate_col_index:\n",
    "    df_cols[i] = df_cols[i] + '_duplicate_'+ str(i)\n",
    "  data = data.toDF(*df_cols)\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5d32a151",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def combine_cosin_df(cosin_accessories_df,cosin_clothing_df,product_tag_df,tag_cloud_df,Product_df,collection_category_tag_df,feature_accesories_df,feature_clothing_df):\n",
    "\n",
    "\n",
    "    product_tag_name_df=product_tag_df.join(tag_cloud_df,product_tag_df.tag_id==tag_cloud_df.id)\n",
    "    product_tag_name_df=product_tag_name_df.join(Product_df,Product_df.product_id==product_tag_name_df.product_id,'inner')\n",
    "    product_tag_name_df=duplicate_col(product_tag_name_df)\n",
    "    product_tag_name_df=product_tag_name_df.select(['product_id','tag_id','id','tag_name','product_title'])\n",
    "    product_tag_name_collection_df=product_tag_name_df.join(collection_category_tag_df,product_tag_name_df.tag_id==collection_category_tag_df.tag_id)\n",
    "    product_accesories_ids=[  int(i) for i in np.unique(product_tag_name_collection_df.filter\n",
    "    (product_tag_name_collection_df['category_id'].isin(\n",
    "        \n",
    "        feature_accesories_df['category_collection_FE']\n",
    "        \n",
    "    )).select('product_id').collect())]\n",
    "    product_clothing_ids=[ int(i)  for i in np.unique(product_tag_name_collection_df.filter\n",
    "    (product_tag_name_collection_df['category_id'].isin(\n",
    "     feature_clothing_df['category_collection_FE']\n",
    "\n",
    "    )).select('product_id').collect())]\n",
    "    \n",
    "    #--testing cosin csv--#\n",
    "        # import pandas as pd\n",
    "        # cosin_clothing_df.to_csv('cosin_clothing_df.csv')\n",
    "        # cosin_accessories_df.to_csv('cosin_accessories_df.csv')\n",
    "    #--testing cosin csv--#\n",
    "    cosin_clothing_df=cosin_clothing_df[~cosin_clothing_df['Product_A'].isin(product_accesories_ids)]\n",
    "    cosin_accessories_df=cosin_accessories_df[~cosin_accessories_df['Product_A'].isin(product_clothing_ids)]\n",
    "    #--wrote for pyspark--#\n",
    "    # cosin_accessories_df=cosin_accessories_df.filter(~cosin_accessories_df['Product_B'].isin(product_clothing_ids))  \n",
    "    # cosin_clothing_df=cosin_clothing_df.filter(~cosin_clothing_df['Product_B'].isin(product_accesories_ids))\n",
    "    # cosin_clothing_df=cosin_clothing_df.toPandas()\n",
    "    # cosin_accessories_df=cosin_accessories_df.toPandas()\n",
    "    #--wrote for pyspark--#\n",
    "    print(cosin_accessories_df.head())\n",
    "    print(cosin_clothing_df.head())\n",
    "    merge_csv=cosin_accessories_df._append(cosin_clothing_df,ignore_index = True)  \n",
    "    merge_csv=merge_csv[['cosin_score',  'Product_A'  ,'Product_B']]\n",
    "    return merge_csv  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1672844c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0.1  Unnamed: 0  cosin_score  Product_A  Product_B\n",
      "0             0           0     0.731022        829       1436\n",
      "1             1           1     0.601156        829       6731\n",
      "2             2           2     0.861249        829       2294\n",
      "3             3           3     0.852268        829       1159\n",
      "4             4           4     0.500400        829        296\n",
      "   Unnamed: 0.1  Unnamed: 0  cosin_score  Product_A  Product_B\n",
      "0             0           0     0.429471       1159       4032\n",
      "1             1           1     0.526363       1159       3606\n",
      "2             2           2     0.404225       1159        675\n",
      "3             3           3     0.386409       1159       2904\n",
      "4             4           4     1.000000       1159       4937\n",
      "   Unnamed: 0.1  Unnamed: 0  cosin_score  Product_A  Product_B\n",
      "0             0           0     0.731022        829       1436\n",
      "1             1           1     0.601156        829       6731\n",
      "2             2           2     0.861249        829       2294\n",
      "3             3           3     0.852268        829       1159\n",
      "4             4           4     0.500400        829        296\n"
     ]
    }
   ],
   "source": [
    "accesories_df=pd.read_csv('C:/Users/Bionic Computer/Downloads/ubaidnizami-fp-ai/ubaidnizami-fp-ai-081beb29d155/cosin_accessories_df.csv')\n",
    "clothing_df=pd.read_csv('C:/Users/Bionic Computer/Downloads/ubaidnizami-fp-ai/ubaidnizami-fp-ai-081beb29d155/cosin_clothing_df.csv')\n",
    "combine_cosin_df(accesories_df,clothing_df,product_tag_df,tag_cloud_df,Product_df,collection_category_tag_df,feature_accesories_df,feature_clothing_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61d85f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# product_A=cosin_clothing_df.select('Product_A').collect()\n",
    "# product_B=cosin_accessories_df.select('Product_A').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30c160d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_A=np.unique(product_A)\n",
    "product_B=np.unique(product_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5e53e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4278 4278\n"
     ]
    }
   ],
   "source": [
    "print(len(product_A),\n",
    "len(product_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62deef57",
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_csv=cosin_accessories_df.union(cosin_clothing_df)\n",
    "combine_csv=combine_csv.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152986c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_csv.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb59c77",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_ALREADY_EXISTS] Path file:/D:/PROJECTS/Recomendation_System_FP/Code/notebook already exists. Set mode as \"overwrite\" to overwrite the existing path.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m combine_csv\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39moptions(header\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrue\u001b[39m\u001b[38;5;124m'\u001b[39m, delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m) \\\n\u001b[1;32m----> 2\u001b[0m  \u001b[38;5;241m.\u001b[39mcsv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:/PROJECTS/Recomendation_System_FP/Code/notebook\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Z2 mini\\.conda\\envs\\RS\\Lib\\site-packages\\pyspark\\sql\\readwriter.py:1799\u001b[0m, in \u001b[0;36mDataFrameWriter.csv\u001b[1;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode(mode)\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[0;32m   1782\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m   1783\u001b[0m     sep\u001b[38;5;241m=\u001b[39msep,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1797\u001b[0m     lineSep\u001b[38;5;241m=\u001b[39mlineSep,\n\u001b[0;32m   1798\u001b[0m )\n\u001b[1;32m-> 1799\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39mcsv(path)\n",
      "File \u001b[1;32mc:\\Users\\Z2 mini\\.conda\\envs\\RS\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\Z2 mini\\.conda\\envs\\RS\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    171\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: [PATH_ALREADY_EXISTS] Path file:/D:/PROJECTS/Recomendation_System_FP/Code/notebook already exists. Set mode as \"overwrite\" to overwrite the existing path."
     ]
    }
   ],
   "source": [
    "combine_csv.write.options(header='True', delimiter=',') \\\n",
    " .csv(\"C:/Users/Bionic Computer/Downloads/ubaidnizami-fp-ai/ubaidnizami-fp-ai-081beb29d155/Code/notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c617da",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o5417.collectToPython.\n: java.lang.OutOfMemoryError: Java heap space\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m cosin_result_path_df1\u001b[38;5;241m=\u001b[39mcombine_csv\u001b[38;5;241m.\u001b[39mtoPandas()\n",
      "File \u001b[1;32mc:\\Users\\Z2 mini\\.conda\\envs\\RS\\Lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:208\u001b[0m, in \u001b[0;36mPandasConversionMixin.toPandas\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    205\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;66;03m# Below is toPandas without Arrow optimization.\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m pdf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_records(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollect(), columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[0;32m    209\u001b[0m column_counter \u001b[38;5;241m=\u001b[39m Counter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[0;32m    211\u001b[0m corrected_dtypes: List[Optional[Type]] \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema)\n",
      "File \u001b[1;32mc:\\Users\\Z2 mini\\.conda\\envs\\RS\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:1216\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1196\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[0;32m   1197\u001b[0m \n\u001b[0;32m   1198\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1213\u001b[0m \u001b[38;5;124;03m[Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\u001b[39;00m\n\u001b[0;32m   1214\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1215\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc):\n\u001b[1;32m-> 1216\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mcollectToPython()\n\u001b[0;32m   1217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001b[1;32mc:\\Users\\Z2 mini\\.conda\\envs\\RS\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\Z2 mini\\.conda\\envs\\RS\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\Z2 mini\\.conda\\envs\\RS\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o5417.collectToPython.\n: java.lang.OutOfMemoryError: Java heap space\r\n"
     ]
    }
   ],
   "source": [
    "cosin_result_path_df1=combine_csv.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e0deb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosin_result_path_df1=combine_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68307a1a",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "product_title='X FASHIONPASS MARCIE EARRINGS'\n",
    "selected_product_id=Product_df.filter(Product_df['product_title']==product_title).select('product_id').collect()[0][0]\n",
    "recomendation_df_product=cosin_result_path_df1[cosin_result_path_df1['Product_A']==selected_product_id]\n",
    "link=[]\n",
    "for row in recomendation_df_product.collect():\n",
    "    if float(row['cosin_score'])>=.80:\n",
    "        id=row['Product_B']\n",
    "        # link.append(int(id))\n",
    "        # product_thumbnail=Product_df.filter(Product_df.product_id==id).select('product_thumbnail').collect()\n",
    "        try:\n",
    "            # print('https://fashionpass.s3.us-west-1.amazonaws.com/products/'+(product_thumbnail[0][0]))\n",
    "             link.append(id)\n",
    "        except:\n",
    "            print('valid string')    \n",
    "print(','.join(link))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d842d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3939,4407,7137,8167,2668,6832,8202,4815,8245,7534,8570,2006,5441,4286,7811,5091,2459,7998,5194,5943,8477,2591,5602,5032,6224,6845,8552,7641,3962,8081,1976,4627,2687,5860,3963,2844,8331,3323,7141,3623,8476,4811,4900,7733,8753,4946,7960,4285,8097,7804,7726,5172,7844,7803,1951,4331,4311,7725,3769,4810,1231,2328,7308,5940,8143,5382,8250,7661,6798,5410,5938,6283,8080,3961,6223,6678,2177,7826,4962,4108,4406,2254,8051,8637,6890,3192,7148,3217,5423,7602,8752,5381,8550,8750,3564,8664,4927,7997,5438,4143,8254,7535,4625,5736,180,4102,1531,316,6432,1450,5201,5719,5741,6975,6231,7619,5939,8316,8076,2549,8075,3759,5424,5422,5368,3878,5405,5288,5611,859,6225,6073,3440,1952,3423,5404,5369,5182,4963,7140,7986,2332,4978,7876,4210,7800,6540,4287,3205,3575,7802,7796,1949,5279,1614,7938,5089,8754,3385,557,8475,2001,915,6421,5858,6610,3749,3725,5281,8487,5126,5171,4145,2399,7172,7660,2707,5185,7943,8751,2114,2029,5131,6265,278,6115,3396,8054,8867,8400,4409,3422,8068,7124,8523,6397,6880,8660,1251\n"
     ]
    }
   ],
   "source": [
    "selected_product_id=Product_df.filter(Product_df['product_title']==product_title).select('product_id').collect()[0][0]\n",
    "recomendation_df_product=cosin_result_path_df2[cosin_result_path_df2['Product_A']==selected_product_id]\n",
    "link=[]\n",
    "for row in recomendation_df_product.collect():\n",
    "    if float(row['cosin_score'])>=.80:\n",
    "        id=row['Product_B']\n",
    "        # link.append(int(id))\n",
    "        # product_thumbnail=Product_df.filter(Product_df.product_id==id).select('product_thumbnail').collect()\n",
    "        try:\n",
    "            # print('https://fashionpass.s3.us-west-1.amazonaws.com/products/'+(product_thumbnail[0][0]))\n",
    "             link.append(id)\n",
    "        except:\n",
    "            print('valid string')    \n",
    "print(','.join(link))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5356ac60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5444,2324,2940,7779,1668,7630,1200,2747,604,104,3599,978,3713,5342,7675,5973,3921,1557,7555,7874,8112,4763,1790,645,5506,1237,1301,1105,1078,8033,3473,7253,3538,1348,1201,8405,1028,7599,1714,1068,5446,7748,1206,1523,7276,5749,8403,1558,7870,8155,4188,7557,7608,281,461,741,1080,350,114,2897,540,759,8192,4958,1597,8032,7965,1768,3598,8102,490,24,7558,757,3356,8141,7744,5391,1778,289,8156,5448,7443,5447,8132,7254,1848,2924,8231,776,3596,3675,5670,8404,7783,3514,49,1661,4425,7610,7265,1242,2320,4181,4254,3710,7871,1233,1029,4251,1553,7206,4486,1350,1273,66,8191,8232,760,2730,8230,8401,5389,8506,7216,1699,6718,1781,2704,3594,5390,7264,8111,2746,3601,2732,5953,4316,145,2076,8188,6137,881,1779,2317,119,8128,8190,1329,8189\n"
     ]
    }
   ],
   "source": [
    "selected_product_id=Product_df.filter(Product_df['product_title']==product_title).select('product_id').collect()[0][0]\n",
    "recomendation_df_product=cosin_result_path_df3[cosin_result_path_df3['Product_A']==selected_product_id]\n",
    "link=[]\n",
    "for row in recomendation_df_product.collect():\n",
    "    if float(row['cosin_score'])>=.80:\n",
    "        id=row['Product_B']\n",
    "        # link.append(int(id))\n",
    "        # product_thumbnail=Product_df.filter(Product_df.product_id==id).select('product_thumbnail').collect()\n",
    "        try:\n",
    "            # print('https://fashionpass.s3.us-west-1.amazonaws.com/products/'+(product_thumbnail[0][0]))\n",
    "             link.append(id)\n",
    "        except:\n",
    "            print('valid string')    \n",
    "print(','.join(link))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3baf6e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "cs1=pd.read_csv(cosin_result_path3)#configur.get('cosin_similarity_base_path','cs1'))#load_csv(spark_session,file_path=configur.get('cosin_similarity_base_path','cs1'))\n",
    "cs2=pd.read_csv(cosin_result_path2)#configur.get('cosin_similarity_base_path','cs2'))#load_csv(spark=spark_session,file_path=configur.get('cosin_similarity_base_path','cs2'))\n",
    "cs3=pd.read_csv(cosin_result_path1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6031e7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'6878'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_title='X CAMELIA FARHOODI THE LEILA CHOKER'\n",
    "selected_product_id=[Product_df.filter(Product_df['product_title']==product_title).select('product_id').collect()[0][0]]\n",
    "(selected_product_id[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbad1351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of product 1\n",
      "6878\n",
      "top re [3638, 3575, 3932, 1949, 5715, 5279, 5089, 4532, 3554, 4103, 2659, 557, 4292, 764, 2394, 915, 4142, 3637, 3725, 5281, 4718, 3211, 6095, 2656, 5171, 5126, 4145, 2399, 4734, 2983, 5466, 2969, 2620, 6265, 5363, 5131, 5127, 8867, 4106, 2590, 1324, 8862, 2398, 6880, 8479, 8133]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# from Code.utils.libraries import * \n",
    "# from Code.data.dataprocessing.load.load import *\n",
    "# from Code.data.dataprocessing.transform.transform import *\n",
    "# from sessions import *\n",
    "# from pyspark.sql.functions import date_format\n",
    "# from Code.models.model_training.model import *\n",
    "\n",
    "\n",
    "# BASE_PATH=os.getenv('BASEPATH')\n",
    "# CONN_URL = os.getenv('CONN_URL')\n",
    "# PASSWORD = os.getenv('PASSWORD')\n",
    "# USERNAME=os.getenv('USER_NAME')\n",
    "# configur = ConfigParser() \n",
    "# configur.read(BASE_PATH+'config.ini')\n",
    "#spark_session=session(configur.get('driver_path_pyspark','pyspark'))\n",
    "#configur.get('cosin_similarity_base_path','cs3'))#Sload_csv(spark=spark_session,file_path=configur.get('cosin_similarity_base_path','cs3'))\n",
    "\n",
    "cosin_df=[]\n",
    "product_title='BLUE CLUTCH'\n",
    "product_ids=[Product_df.filter(Product_df['product_title']==product_title).select('product_id').collect()[0][0]]#[(Product_df.filter(Product_df['product_title']==product_title).select('product_id').collect()[0][0])]#cs1.Product_A.unique()#[id[0] for id in cs1.select('Product_A').distinct().collect()]  in pyspark [8692,2178,8687,1087,3069]\n",
    "print(\"total number of product\",len(product_ids))\n",
    "# st = time.time()\n",
    "\n",
    "for r in product_ids:\n",
    "    r=int(r)\n",
    "    print(r)\n",
    "    cs1_product_b=[]\n",
    "    cs2_product_b=[]\n",
    "    cs3_product_b=[]\n",
    "\n",
    "    cs1_product_b=cs1[(cs1['Product_A']==r) & (cs1['cosin_score']>=.80)]['Product_B'].unique().tolist()  #[ product[0] for product in cs1.filter((cs1['Product_A']==r) & (cs1['cosin_score']>=.80))['Product_B'].unique()]\n",
    "    cs2_product_b=cs2[(cs2['Product_A']==r) & (cs2['cosin_score']>=.85)]['Product_B'].unique().tolist()#values\n",
    "    cs3_product_b=cs3[(cs3['Product_A']==r) & (cs3['cosin_score']>=.90)]['Product_B'].unique().tolist()#values\n",
    "    # cs2temp=cs2.filter((cs2['Product_A']==r) & (cs2['cosin_score']>=.85))\n",
    "    # cs3temp=cs3.filter((cs3['Product_A']==r) & (cs3['cosin_score']>=.90))\n",
    "    # print(cs1_product_b)\n",
    "    # print(cs2_product_b)\n",
    "    # print(cs3_product_b)\n",
    "\n",
    "    # for row in  cs1temp.collect():\n",
    "    #     if float(row['cosin_score'])>=.80:\n",
    "    #         cs1_product_b.append(row['Product_B'])\n",
    "        \n",
    "    # for row2 in cs2temp.collect():\n",
    "    #     if float(row2['cosin_score'])>=.85:\n",
    "    #         cs1_product_b.append(row2['Product_B'])\n",
    "    #         # cs2_product_b.append(row2['Product_B'])\n",
    "    # for row3 in cs3temp.collect():\n",
    "    #     if float(row3['cosin_score'])>=.90 :\n",
    "    #         cs1_product_b.append(row3['Product_B'])\n",
    "                # cs3_product_b.append(row3['Product_B'])\n",
    "\n",
    "#print(cs1_product_b,cs2_product_b,cs3_product_b)\n",
    "    for id in cs2_product_b:\n",
    "        cs1_product_b.append(id)\n",
    "    for id in cs3_product_b:\n",
    "        cs1_product_b.append(id)\n",
    "    # final_list=cs1_product_b+cs2_product_b+cs3_product_b\n",
    "    my_dict = {i:cs1_product_b.count(i) for i in cs1_product_b}\n",
    "    # print(my_dict)\n",
    "    top_recomendation=[]\n",
    "    for key,value in my_dict.items():\n",
    "        if value >=2:\n",
    "            top_recomendation.append(key)\n",
    "    print('top re',top_recomendation)\n",
    "    cosin_df.append(cs1[(cs1['Product_A']==r) & (cs1['Product_B'].isin(top_recomendation))])\n",
    "    # cosin_df.append(cs1.filter((cs1['Product_A']==r) & (cs1['Product_B'].isin(top_recomendation))))  # in pandas\n",
    "\n",
    "df=cosin_df[0]\n",
    "print(len(cosin_df[1:]))\n",
    "for i in cosin_df[1:]:\n",
    "    #  try:\n",
    "         #df=df.union(i) #in pyspark\n",
    "         df=df._append(i,ignore_index = True) #in pandas\n",
    "    #  except:\n",
    "    #      print('out of index')\n",
    "#df.toPandas().to_csv('cosin_simmilarity1.csv')\n",
    "# et = time.time()\n",
    "# print(f'Execution time:{et-st} seconds')\n",
    "# df.to_csv('cosin_simmilarity2.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
